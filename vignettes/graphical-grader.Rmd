---
title: "Using the `{devoirs}` graphical grader"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using the devoirs graphical grader}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r include=FALSE}
library(devoirs)
```

Preliminary nomenclature:

`{devoirs}`
: An R package that works in conjunction with Quarto to produce interactive html files

`{devoirs}` document
: A `.qmd` source file that is compiled to HTML. 

`{devoirs}` HTML file
: The file produced by compiling the `{devoirs}` document. You should arrange for these HTML file to be available to your students by placing them on a web site. 

`webr` chunk
: An item in a `{devoirs}` document that is similar to an ordinary Rmarkdown R chunk but uses the identifier `:::{webr-r}` instead of `:::{r}`. Students can edit and run R commands in `webr` chunks while viewing the HTML-version of the document.

`MCQ`
: The identifier used for individual interactive multiple-choice questions witin a `{devoirs}` document.

Essay
: Another interactive element within a `{devoirs}` document that is suitable for free-form text answers.

JSON string
: The format used by `{devoirs}` HTML files to collect student answers to the questions and webr chunks in a document. We will also use "JSON string" or "submission" to refer to the contents of this string.

collection site
: A facility, set up by the course instructor, where students can upload (or simply "paste") the JSON string from their work with course HTML files.

`{devoirs}` course
: Consists of at least two items: a set of one or more `{devoirs}` HTML files and a collection site.

`{devoirs}` grading directory
: A directory (set up in a way described below) for assembling student submissions, grading them, and summarizing the grades in a gradebook.


MCQs, Essays, and `webr` chunks embedded in a `{devoirs}` html file are linked to a "Collect your answers" button. Pressing this button causes relevant information to be assembled (using javascript in the html file) and formatted as JSON. The formatted information is automatically copied to the user's clipboard.

Instructors should set up collection site where students submit the JSON string to a site of their own choosing. I find that a Google Form provides a nice site. An example is [here](https://docs.google.com/forms/d/e/1FAIpQLSevJgyHTRHm6_HVQqrYMd8wjMYifPknPLXtpVTunlWlO7RbMw/viewform), but do not use this for your own students because you do not have read permissions on the corresponding spreadsheet. So set up your own Google Form site and generate the web link to the submissions form (which will be student facing) and another web link (which only graders will need) which points to the Google-Form published CSV file. (You need to use the "export as" feature in the Google Forms editor to produce this.)

Put the student-facing collection site link on your course web site or even within your `{devoirs}` document. Write down the Google Form CSV link for your `{devoirs}` grading directory.

## Creating a collection site

`{devoirs}` documents have a "Collect your answers" buttons that copies the student answers for that session of that document to the student's clipboard. The "collection site" is a place for the student to paste from the clipboard to submit the answers. You create your own collection site, using whatever system is appropriate. (Google Forms works well, but you may prefer some institutional site.) That system needs to collect a timestamp, an identifier for the student (e.g. an email address), and the text that the student has pasted from the clipboard. (Note that the text is formatted as JSON and therefore looks like gibberish to a human reader. Tell your students not to clean it up!)

To work with the graphical grader, the new submissions need to be made available as a CSV file. For example, with Google Forms, you can collect the submissions into a Google Sheet, which you can then publish to the web and access via a URL generated by Google. (Security comes from not giving the URL to anyone who shouldn't have it, like students!) Other collection systems have their own ways of providing such functionality, but even something as primitive as downloading a CSV to your laptop will work. 

This collection-site CSV URL or file name will be the one you use for the `submissions-file` field discussed under "Creating a grading directory."

## Creating a grading directory {#sec-grading-directory}

Typically you will have a single grading directory for each course, or perhaps each section of a course or some other unit of course activity. From a student's point of view, the course corresponds to the collection site link. From the instructor's point of view, the course also includes the grading directory. Only one "course" can be in each grading directory. Use multiple grading directories when you have multiple courses.

In typical use, an instructor would set up a new grading directory each semester for each course that uses `{devoirs}`. This is a one-time operation.

You can use a convenience function to set up the grading directory. Of course, replace `"my_course_name"` with your actual course name.

```r
devoirs::create_grading_directory("my_course_name")
```

A grading directory consists initially of one file and one sub-directory. You make the grading directory only once.

- `course-parameters.yml` a YAML file describing the course name, the link to the Google Forms spreadsheet file, and the roster of identifiers for individual students in the course. (I use the students' email addresses for this roster.)
- A sub-directory named `Score_files`.

To construct the link to the Google Forms spreadsheet file, go from the Google Form editor to the spreadsheet view of the submissions. In the upper-right corner there is a share button. The main part of the button allows you to grant access to other instructors (if needed). The drop-down part of the button has a "copy link" item. Press that to get a URL for the spreadsheet, then paste that as a quoted string in the `submissions-file` field of the `course-parameters.yml` document.

When you start using the grading directory, particularly when you call `update_submissions()`, another file will be created:

- `Permanent_store.RDS` a file established and maintained by `{devoirs}` to hold all of the student submissions that have been downloaded from the collection site. You do not need to work with this file directly. 

## Opening the graphical grader

You must use the graphical grader from within RStudio. Make sure the "Console" is visible and give this command:

```r
devoirs::open_graphical_grader()
```

This will display a dialog box to allow you to select a grading directory. (If you haven't yet made the grading directory for your course, see @sec-grading-directory.) Then a browser window will be opened containing the graphical grader. There is a menu at the top of the window with several tabs: [select document], [score MC], [MC diagnostics], [Essays], and [R chunks]. The [select document] tab will be open at first. 

If the course name is not correctly display, for some reason, you selected an invalid directory in the `open_graphical_grader()` dialog, you can fix things with the "Select

## Select Document: [select document]

The [select document] allows you to choose a document from your course. Check that the displayed course name is correct. If not, press the "Select grading directory" button and navigate to the correct directory for the course you want to grade. 

Initially, there may not be any documents listed in the drop-down menu. That will be case case if no submissions have been downloaded from the collection site. Press "**Get new submissions**" to download any submissions currently stored on the collection site. 

IMPORTANT NOTE: If reading from a Google Forms collection site (the only kind of collection site currently supported), you will need to authenticate yourself. The message to do this appears in the **console** of RStudio, not in the graphical grading app. In the console, select whichever option is appropriate.  

The "Select Document" drop down menu allows you to pick a document from your course. Only documents for which submissions have already been made will be displayed. 

Once you have selected a document, the submissions made from that document will be automatically collated. You can display them by selecting one of [Score MC], [Essays], or [R chunks].

## Multiple-choice diagnostics: [MC Diagnostics] {#sec-mc-diagnostics}

If there were multiple-choice questions in the document for which answers have been submitted, a table will be displayed indicating for each multiple-choice item ID how many students answered, how many of these answers were collect, etc. 

If you see an item for which a suspiciously low fraction of answers were incorrect, you may want to check the source materials for your course to see if the question was ill-framed or is otherwise faulty. (Do a global search in the RStudio editor for the item ID.) You may want to correct the source materials, but this will not change the student submissions. Such changes take effect only when you recompile the relevant document, deploy it on your course site, and students create new submissions from the updated document. Note that a student submitting a new answer to such a revised item will have that item scored, since the chronologically last submission for each item is used.

## Scoring multiple-choice questions: [Score MC]

Multiple-choice items are scored either correct or incorrect per the information provided in the document's source. The display in the [Score MC] tab lists the students who have made submissions *and are in the class-list* from the Course-parameters.yml file in the grading directory. For each student, the table lists the total number of answers submitted and the number of those that are correct. Multiple submissions by a student are automatically de-duplicated in favor of the most recent submission for each item. You don't need to intervene, except ....

If you have determined that a question item was faulty, you can manually add compensatory submission points. Similarly, if a student makes a good case, you can manually add compensatory points. 

::: {.callout-note}
## Save your scores! {#sec-save-scores}
Make sure to press the "Save Scores" button after you have viewed the or modified the scores. The same button appears on the [Essays] and [R chunks] tabs. Pressing any of them will save the scores from all three tabs: [Score MC], [Essays], and [R chunks]. 
:::

## Scoring essay answers: [Essays]

The essay display is fundamentally different from the [Score MC] display. First, it relates to the `devoirs_text()` item in the document. Every item for which a student (in the *class-list*) made a submission is listed on its own line. (Multiple submissions are automatically de-duplicated in favor of the most recent submission.)

Essays are not scored automatically. It's anticipated that later versions of the graphical grader will allow you to apply grading heuristics that will generate a score 0-3 for each essay item for each student. Until then, scoring is a matter of manually applying a score to each item. Sorting the essay score table by `itemid` will juxtapose all students' answers to each question, making it easier to score them.

Remember to [Save your scores](#sec-save-scores)!

## Scoring R chunks: [R chunks]

Scoring here is done in much the same way as for [Essays]. Heuristics may become available in later versions of the graphical grader.

Remember to [Save your scores](#sec-save-scores)!

## Saved scores

[Saving your scores](#sec-save-scores) for a document creates a corresponding file in the `Score_files` subdirectory of your grading directory. 

Whenever you re-open a document in the graphical grading , the previous scores for each item will be read from the saved file and the scoring tabs will display those scores. In this way, you can score incrementally.

## Tabularing scores into a grade-book

Another app will be made available to read the scores stored in the `Score_files` directory, weight them, and construct a gradebook file. 

